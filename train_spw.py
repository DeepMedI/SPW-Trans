import argparse
import os
import torch
from torch.optim import Adam
from torch.optim.lr_scheduler import StepLR
import losses1
from data_util.ctscan import sample_generator

import matplotlib
matplotlib.use('AGG')
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import numpy as np
import json
import sys
sys.path.append('/home/hr@04/medical/ViT/ViT/liver_trian/pytorch_VTN/Recursive-Cascaded-Networks-master/')
import liver 
from liver import Dataset
from spwnet import ViTVNet
import configs3d as configs
from tqdm import tqdm
import cv2


parser = argparse.ArgumentParser()
parser.add_argument('--b', "--batch_size", type=int, default=1)
parser.add_argument('--e', "--n_epochs", type=int, default=200)
parser.add_argument('--gpu', "--n_gpu", type=str,default="0")
parser.add_argument('--i', "--n_iters_train", type=int, default=2000)
parser.add_argument('--iv', "--n_iters_val", type=int, default=400)
parser.add_argument('--c', "--checkpoint_frequency", type=int, default=1)
parser.add_argument('--fixed_sample', type=int, default=100)
args = parser.parse_args()


class Split:
    TRAIN = 1
    VALID = 2

def save_CT_to_png(CT_, save_path):
    sum_slice = CT_.shape[0]
    #print(sum_slice)
    for slice_i in tqdm(range(sum_slice)):
        slice_ = CT_[:, :,slice_i].copy()
        slice_ = np.uint8(slice_)
        cv2.imwrite(save_path + '/' + str(slice_i) + '.png', slice_)

def identify_axes(ax_dict, fontsize=48):
    kw = dict(ha="center", va="center", fontsize=fontsize, color="darkgrey")
    for k, ax in ax_dict.items():
        ax.text(0.5, 0.5, k, transform=ax.transAxes, **kw)


def plot_grid(ax, flow, factor=10):
    """ Plot the grid generated by a flow. The displacement can be too small, so we add a scale factor"""
    grid = factor * flow[:, ::8, ::8]
    lin_range = np.linspace(0, 512, 64)
    x, y = np.meshgrid(lin_range, lin_range)
    x = x + grid[0, ...]
    y = y + grid[1, ...]
    y = y

    segs1 = np.stack((x, y), axis=2)
    segs2 = segs1.transpose(1, 0, 2)
    ax.add_collection(LineCollection(segs1, color='black', linewidths=0.8))
    ax.add_collection(LineCollection(segs2, color='black', linewidths=0.8))
    ax.autoscale()


def generate_plots(fixed, moving, warped, flows, train_loss, val_loss, reg_loss, epoch):
    """ Save some images and plots during training"""
    print(moving.shape)
    moving = moving[:,:,58,:, :].detach().cpu().numpy()
    fixed = fixed[:,:,58,:, :].detach().cpu().numpy()
    warped = [w.detach().cpu().numpy() for w in warped]
    flows = [f.detach().cpu().numpy() for f in flows]

    fig = plt.figure(constrained_layout=True, figsize=(4 * 5, 4 * 3))
    ax_dict = fig.subplot_mosaic('''
                                 FABCD
                                 LGHIE
                                 MKJWX
                                 ''')
    ax_dict['F'].imshow(moving[0, 0, ...], cmap='gray')
    ax_dict['F'].set_title('Moving')
    ax_dict['W'].imshow(fixed[0, 0, ...], cmap='gray')
    ax_dict['W'].set_title('Fixed')

    for i, ax_name in enumerate(list("ABCDEX")):
        ax_dict[ax_name].imshow(warped[i][0, 0, ...], cmap='gray')
        if ax_name == "A":
            ax_dict[ax_name].set_title("Affine")
        else:
            ax_dict[ax_name].set_title(f"Cascade {i}")

    ax_dict['L'].plot(train_loss, color='red', label='train_loss')
    ax_dict['L'].plot(val_loss, label='val_loss', color='blue')
    ax_dict['L'].plot(reg_loss, label='train_reg_loss', color='green')
    ax_dict['L'].set_title("Losses")
    ax_dict['L'].grid()
    ax_dict['L'].set_xlim(0, args.e)
    ax_dict['L'].legend(loc='upper right')
    ax_dict['L'].scatter(len(train_loss) - 1, train_loss[-1], s=20, color='red')
    ax_dict['L'].scatter(len(val_loss) - 1, val_loss[-1], s=20, color='blue')
    ax_dict['L'].scatter(len(reg_loss) - 1, reg_loss[-1], s=20, color='green')

    for i, ax_name in enumerate(list("GHIJKM")):
        plot_grid(ax_dict[ax_name], flows[i][0, ...])
        if ax_name == "G":
            ax_dict[ax_name].set_title("Affine")
        else:
            ax_dict[ax_name].set_title(f"Cascade {i}")

    plt.suptitle(f"Epoch {epoch}")
    plt.savefig(f'./ckp/visualization/epoch_{epoch}.png')


def main():
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    device = "cuda"
    save_path = './ckp_ncc_grad06_ncc16/'
    save_path_sample = save_path+'sample_save/'
    
    if not os.path.exists(save_path+'model_wts'):
        print("Creating ckp dir")
        os.makedirs(save_path+'model_wts')

    if not os.path.exists(save_path+'visualization'):
        print("Creating visualization dir")
        os.makedirs(save_path+'visualization')
        
    if not os.path.exists(save_path_sample):
        print("Creating sample_save dir")
        os.makedirs(save_path_sample)
    
    fixed_save_path = save_path_sample+'fixed_save_path'
    if not os.path.exists(fixed_save_path):
        print("Creating fixed_save_path")
        os.makedirs(fixed_save_path) 
    
    moving_save_path = save_path_sample+'moving_save_path'
    if not os.path.exists(moving_save_path):
        print("Creating fixed_save_path")
        os.makedirs(moving_save_path)
        
    warped_save_path = save_path_sample+'warped_save_path'
    if not os.path.exists(warped_save_path):
        print("Creating /warped_save_path")
        os.makedirs(warped_save_path)   
    
    sim_loss_fn = losses1.MutualInformation
    sim_loss_fn1 = losses1.NCC
    grad_loss_fn = losses1.Grad3d
        
    CONFIGS = {'ViT-V-Net': configs.get_3DReg_config(),}
    config_vit = CONFIGS['ViT-V-Net']
    model = ViTVNet(config_vit, img_size=(128, 128, 128))
    model.to(device)
    #print(model)
    #model = ViTVNet(n_cascades=args.n, im_size=(128, 512))
    #trainable_params = []
    #for submodel in model.stems:
        #trainable_params += list(submodel.parameters())

    #trainable_params += list(model.reconstruction.parameters())

    optim = Adam(model.parameters(), lr=1e-4)
    scheduler = StepLR(optimizer=optim, step_size=10, gamma=0.96)
    #train_generator = iter(sample_generator('./train.txt', batch_size=args.b))
    #val_generator = iter(sample_generator('./validation.txt', batch_size=args.b))
    with open(os.path.join('/home/hr@04/medical/ViT/ViT/liver_trian/pytorch_VTN/Recursive-Cascaded-Networks-master/datasets/liver.json'), 'r') as f:
        cfg = json.load(f)
        image_size = cfg.get('image_size', [128, 128, 128])
        image_type = cfg.get('image_type')

    dataset = Dataset('/home/hr@04/medical/ViT/ViT/liver_trian/pytorch_VTN/Recursive-Cascaded-Networks-master/datasets/liver.json', '/home/hr@04/medical/ViT/ViT/liver_trian/pytorch_VTN/Recursive-Cascaded-Networks-master/datasets/')
    if None is not None:
        if 'finetune-train-%s' % args.finetune in dataset.schemes:
            dataset.schemes[Split.TRAIN] = dataset.schemes['finetune-train-%s' %
                                                           args.finetune]
        if 'finetune-val-%s' % args.finetune in dataset.schemes:
            dataset.schemes[Split.VALID] = dataset.schemes['finetune-val-%s' %
                                                           args.finetune]
        print('train', dataset.schemes[Split.TRAIN])
        print('val', dataset.schemes[Split.VALID])
    generator = iter(dataset.generator(Split.TRAIN, batch_size=args.b, loop=True))
    generator_val = iter(dataset.generator(Split.VALID, batch_size=args.b, loop=True))

    # Saving the losses
    train_loss_log = []
    reg_loss_log = []
    val_loss_log = []

    for epoch in range(1, args.e + 1):
        print(f"-----Epoch {epoch} / {args.e}-----")
        train_epoch_loss = 0
        train_reg_loss = 0
        vis_batch = []
        model.train()
        for iteration in range(1, args.i):
            if iteration % int(0.1 * args.i) == 0:
                print(f"\t-----Iteration {iteration} / {args.i} -----")
            optim.zero_grad()
            #fixed, moving = next(generator)
            data = next(generator)
            fixed = torch.tensor(data['voxel1'])
            moving = torch.tensor(data['voxel2'])
            fixed = fixed.cuda()
            fixed = fixed.permute(0, 4, 1, 2, 3)
            #print(fixed.shape)
            moving = moving.cuda()
            moving = moving.permute(0, 4, 1, 2, 3)
            data = torch.cat((moving,fixed),1)
            warped, flows = model(data)
            #print(warped.shape)
            #print(flows.shape)

            #loss = total_loss(fixed, warped[-1], flows)
            #sim, reg = total_loss(fixed, warped, flows)
            recon_loss1 = sim_loss_fn1(fixed, warped)
            grad_loss = grad_loss_fn(flows)
            #print(recon_loss1.type)
            #print(grad_loss.type)
            loss = recon_loss1 + grad_loss*0.6
            #loss = sim + reg
            loss.backward()
            optim.step()
            

            train_epoch_loss += loss.item()
            train_reg_loss += grad_loss.item()

            if iteration == args.fixed_sample:
                vis_batch.append(fixed)
                vis_batch.append(moving)
                vis_batch.append(warped)
                vis_batch.append(flows)
                
                
        fixed_png = np.squeeze(fixed.cpu().detach().numpy())#np.squeeze()
        moving_png = np.squeeze(moving.cpu().detach().numpy())
        warped_png = np.squeeze(warped.cpu().detach().numpy())
        save_CT_to_png(fixed_png,fixed_save_path)
        save_CT_to_png(moving_png,moving_save_path)
        save_CT_to_png(warped_png,warped_save_path)
        
        train_loss_log.append(train_epoch_loss / args.i)
        reg_loss_log.append(train_reg_loss / args.i)

        model.eval()
        print(f">>>>>  <<<<<")

        val_epoch_loss = 0
        for iteration in range(1, args.iv):
            if iteration % int(0.1 * args.iv) == 0:
                print(f"\t-----Iteration {iteration} / {args.iv} -----")

            with torch.no_grad():
                data = next(generator_val)
                fixed = torch.tensor(data['voxel1'])
                moving = torch.tensor(data['voxel2'])
                fixed = fixed.cuda()
                fixed = fixed.permute(0, 4, 1, 2, 3)
                moving = moving.cuda()
                moving = moving.permute(0, 4, 1, 2, 3)
                data = torch.cat((moving,fixed),1)
                warped, flows = model(data)
                #warped, flows = model(fixed, moving)
                #sim, reg = total_loss(fixed, warped, flows)
                #loss = sim + reg
                recon_loss1 = sim_loss_fn1(fixed, warped)
                grad_loss = grad_loss_fn(flows)
                loss = recon_loss1 + grad_loss*0.6
                val_epoch_loss += loss.item()

        val_loss_log.append(val_epoch_loss / args.iv)

        scheduler.step()

        if epoch % args.c == 0:
            ckp = {}
            #for i, submodel in enumerate(model.stems):
            ckp[f"model"] = model.state_dict()

            ckp['train_loss'] = train_loss_log
            ckp['val_loss'] = val_loss_log
            ckp['epoch'] = epoch

            torch.save(ckp, f'./ckp_ncc_grad06_ncc16/model_wts/epoch_{epoch}.pth')
        print("train_loss_log,val_loss_log, reg_loss_log, epoch",train_loss_log, val_loss_log, reg_loss_log, epoch)
        #generate_plots(vis_batch[0], vis_batch[1], vis_batch[2], vis_batch[3], train_loss_log, val_loss_log, reg_loss_log, epoch)


if __name__ == "__main__":

    main()